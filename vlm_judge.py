import os
import json
import yaml
import argparse
import re
import torch
from PIL import Image
from vllm import LLM, SamplingParams

# Qwen2-VL / Qwen3-VL æ ‡å‡†å›¾åƒå ä½ç¬¦
IMAGE_PLACEHOLDER = "<|vision_start|><|image_pad|><|vision_end|>"

# ===================== Prompt Templates =====================

# ç›´æ¥æ‰“åˆ† Prompt (åŒå›¾æ¨¡å¼)
DIRECT_SCORE_TEMPLATE = """
Role: Professional Image Editing Judge.

Input:
- Image 1: Reference Original Image (Real Photograph).
- Image 2: AI Generated/Edited Image (Target for evaluation).
- Editing Instruction: "{prompt}"

Task: Evaluate ONLY "Image 2" based on how well it executes the instruction relative to Image 1.

Evaluation Criteria:
1. Instruction Adherence: Did the requested change happen in Image 2?
2. Consistency: Are the un-edited parts of Image 2 consistent with Image 1?
3. Visual Quality: Rate the realism and quality of Image 2.

IMPORTANT: 
- Image 1 is a real photo and has perfect quality. DO NOT let Image 1's quality bias the score of Image 2.
- If Image 2 is blurry, distorted, or fails the prompt, give a low score even if Image 1 looks good.
- Score represents the quality and success of Image 2 ONLY.

Output JSON format ONLY:
{{
    "reasoning": "Step-by-step analysis comparing Image 2 to instruction and Image 1...",
    "score": <0-10 integer>
}}
"""


def load_config(config_path):
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def extract_json(text):
    try:
        return json.loads(text)
    except:
        match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL) or \
                re.search(r'```json\s*(\[.*?\])\s*```', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass
        match = re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except:
                pass
    return None


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="edit_pipeline/config.yaml")
    parser.add_argument("--input_json", required=True, help="Testset JSON generated by build_testset.py")
    parser.add_argument("--debug", action="store_true", help="Run on first 5 items only")
    args = parser.parse_args()

    cfg = load_config(args.config)
    judge_cfg = cfg.get('judge', {})

    with open(args.input_json, 'r') as f:
        test_data = json.load(f)

    if args.debug:
        test_data = test_data[:5]

    # --- å¤šå¡å¹¶è¡Œé€»è¾‘ ---
    available_gpus = torch.cuda.device_count()
    # ä¼˜å…ˆè¯»å– config ä¸­çš„é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ˜¾å¡
    # tp_size = judge_cfg.get('tensor_parallel_size', available_gpus)
    # if tp_size > available_gpus:
    #     print(
    #         f"âš ï¸ Warning: tensor_parallel_size ({tp_size}) > available GPUs ({available_gpus}). Using {available_gpus}.")
    tp_size = available_gpus

    print(f"ğŸš€ Initializing VLLM (Multi-Image Mode) with TP_SIZE={tp_size}...")

    llm = LLM(
        model=judge_cfg.get('model_path'),
        limit_mm_per_prompt={"image": 2},
        tensor_parallel_size=tp_size,
        gpu_memory_utilization=judge_cfg.get('gpu_memory_utilization', 0.9),
        max_model_len=7000,
        trust_remote_code=True
    )
    sampling_params = SamplingParams(temperature=0.1, max_tokens=1024)

    # === Phase: Visual Evaluation (Direct Scoring Only) ===
    print("ğŸ‘ï¸ Running Direct Scoring Evaluation...")

    inputs_direct = []
    valid_indices = []

    for idx, item in enumerate(test_data):
        gen_path = item.get('last_frame_path')
        ref_path = item.get('first_frame_path')

        if not gen_path or not os.path.exists(gen_path):
            print(f"âš ï¸ Generated image missing: {gen_path}")
            continue

        try:
            img_gen = Image.open(gen_path).convert("RGB")
            images = [img_gen]

            has_ref = False
            if ref_path and os.path.exists(ref_path):
                img_ref = Image.open(ref_path).convert("RGB")
                images = [img_ref, img_gen]
                has_ref = True

            valid_indices.append(idx)

            if has_ref:
                img_tokens = f"Image 1 (Ref): {IMAGE_PLACEHOLDER}\nImage 2 (Gen): {IMAGE_PLACEHOLDER}"
            else:
                img_tokens = f"Image 2 (Gen): {IMAGE_PLACEHOLDER}"

            text_direct = DIRECT_SCORE_TEMPLATE.format(prompt=item['prompt'])
            prompt_direct = f"<|user|>\n{img_tokens}\n{text_direct}\n<|assistant|>\n"

            inputs_direct.append({
                "prompt": prompt_direct,
                "multi_modal_data": {"image": images}
            })

        except Exception as e:
            print(f"âŒ Error loading images for {item.get('test_id')}: {e}")

    # æ‰¹é‡æ¨ç†
    outputs_direct = []
    if inputs_direct:
        print(f"   Processing {len(inputs_direct)} items...")
        outputs_direct = llm.generate(inputs_direct, sampling_params)

    # ç»“æœå›å¡«
    for i, list_idx in enumerate(valid_indices):
        item = test_data[list_idx]
        if i < len(outputs_direct):
            item['eval_direct'] = extract_json(outputs_direct[i].outputs[0].text)

    # ä¿å­˜ç»“æœ
    output_file = args.input_json.replace(".json", "_judged_direct.json")
    with open(output_file, 'w') as f:
        json.dump(test_data, f, indent=4, ensure_ascii=False)

    print(f"âœ… Evaluation Complete. Saved to: {output_file}")


if __name__ == "__main__":
    main()